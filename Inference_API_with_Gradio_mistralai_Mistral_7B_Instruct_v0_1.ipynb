{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtdGVWUejhhHAu42O3DszC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlessandroDiLauro/Chat-with-any-LLMs---/blob/main/Inference_API_with_Gradio_mistralai_Mistral_7B_Instruct_v0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral + HF Gradio Chat with any LLMs ü§ò\n",
        "\n",
        "###  Using the new powerful [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) and [Gradio](https://www.gradio.app/) Build & Share LLM Apps.\n",
        "\n",
        "\n",
        "\n",
        "<a target=\"_blank\" href=\"https://gradio.app\">\n",
        "<img src=\"https://huggingface.co/front/assets/spaces-launch-page/gradio-logo.svg\" width=\"270\" style=\"display:inline;\">\n",
        "<div align=\"left\">\n",
        "\n",
        "<em>Build & share delightful machine learning apps easily</em>\n",
        "</a>\n",
        "\n",
        "\n",
        "<img src=\"https://techcrunch.com/wp-content/uploads/2023/09/mistral-7b-v0.1.jpg\" width=\"270\" style=\"display:inline;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<div align=\"center\">\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JqUZTiyrF7zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # 'gradio' is the name of the package you want to install.\n",
        " #Gradio makes it easy to create UIs for prototyping your machine learning model or for creating an interface for it.\n",
        "%%capture\n",
        "pip install gradio"
      ],
      "metadata": {
        "id": "-NgYEeZx0liA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests # The 'requests' library is imported. This library allows you to send HTTP requests using Python.\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload): # The 'query' function is defined. This function sends a POST request to the API URL with the provided headers and payload (data\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": \"Gli LLms sono fantastici perch√© riescono\",\n",
        "})"
      ],
      "metadata": {
        "id": "xAtKl5-JD8BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ6wzBZF0n-1",
        "outputId": "b3c6c125-d6fb-4431-8592-5b34f36f62b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Gli LLms sono fantastici perch√© riescono a capire e rispondere a qualsiasi domanda che ti sia stata posta'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# The 'generate' function is defined. This function takes an input string,\n",
        "#sends it to the Hugging Face Inference API, and returns the generated text.\n",
        "def generate(input):\n",
        "    output = query({\n",
        "\t\"inputs\": input,\n",
        "})\n",
        "    return output[0]['generated_text']\n",
        "\n",
        "gr.close_all()\n",
        "demo = gr.Interface(fn=generate, inputs=\"text\", outputs=\"text\") # A new Gradio Interface is created. The 'generate' function is passed as the function to run when the interface receives input. The types of the input and output are both set to \"text\".\n",
        "demo.launch(share=True) # The 'launch' method of the Gradio Interface is called. This method starts the interface. The 'share' parameter is set to True, which means that a publicly shareable link to the interface will be generated."
      ],
      "metadata": {
        "id": "Lh-YfMZz0oBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "pip install text_generation"
      ],
      "metadata": {
        "id": "GSis1g1Y4pMd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function\n",
        "import requests, json\n",
        "from text_generation import Client\n",
        "\n",
        "\n",
        "# Create an instance of the client, specifying the model to use\n",
        "#Mistral-7B-Instruct endpoint on the text_generation library\n",
        "client = Client(  \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\", headers=headers, timeout=120)"
      ],
      "metadata": {
        "id": "awBBs1884AAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Has math been invented or discovered?\"\n",
        "\n",
        "# 'client.generate' is a method provided by the 'text_generation' package.\n",
        "# It takes in a prompt and generates a response using the specified model.\n",
        "# 'max_new_tokens=200' is an argument that limits the length of the generated text to 200 tokens\n",
        "client.generate( prompt, max_new_tokens=200).generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s-45qHHl5mSm",
        "outputId": "cfa2f802-5eba-49d3-c8ef-710748c82b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAnswer: Discovered'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building an app to chat with any LLM!"
      ],
      "metadata": {
        "id": "qgiiEzHh7UgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import gradio as gr\n",
        "def generate(input, slider):\n",
        "    output = client.generate(input, max_new_tokens=slider).generated_text\n",
        "    return output\n",
        "\n",
        "# Create a Gradio interface for the 'generate' function.\n",
        "# The interface has two inputs: a textbox for the prompt and a slider for the maximum number of new tokens.\n",
        "# The output is a textbox that will display the generated text.\n",
        "demo = gr.Interface(fn=generate, inputs=[gr.Textbox(label=\"Prompt\"), gr.Slider(label=\"Max new tokens\", value=20,  maximum=1024, minimum=1)], outputs=[gr.Textbox(label=\"Completion\")])\n",
        "gr.close_all()\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "h2lMVBXg5mcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gr.Chatbot() to the rescue!"
      ],
      "metadata": {
        "id": "OLngFAiO8TF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to format the chat prompt.\n",
        "def format_chat_prompt(message, chat_history):\n",
        "    # Initialize an empty string for the prompt.\n",
        "    prompt = \"\"\n",
        "     # Iterate over the chat history.\n",
        "    for turn in chat_history:\n",
        "        # Each turn in the chat history is a tuple containing a user message and a bot message.\n",
        "        user_message, bot_message = turn\n",
        "        # Add the user message and bot message to the prompt\n",
        "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
        "    # Add the current user message to the prompt and prepare for the assistant's response.\n",
        "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
        "    return prompt\n",
        "\n",
        "# Define a function to generate a response.\n",
        "def respond(message, chat_history):\n",
        "        # Format the prompt using the current message and chat history.\n",
        "        formatted_prompt = format_chat_prompt(message, chat_history)\n",
        "        # Generate a response using the formatted prompt.\n",
        "        bot_message = client.generate(formatted_prompt,\n",
        "                                     max_new_tokens=1024,\n",
        "                                     stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"]).generated_text\n",
        "        chat_history.append((message, bot_message))\n",
        "        return \"\", chat_history\n",
        "\n",
        "\n",
        "# Start a Gradio Blocks context. This allows you to group multiple Gradio components together\n",
        "with gr.Blocks() as demo:\n",
        "    # Create a Chatbot component. This will display the chat history.\n",
        "    # The 'height=240' argument sets the height of the chatbot window\n",
        "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
        "    msg = gr.Textbox(label=\"Prompt\")\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
        "\n",
        "    # Set the 'click' event of the button to call the 'respond' function.\n",
        "    # The inputs to the function are the 'msg' and 'chatbot' components.\n",
        "    # The outputs of the function are also the 'msg' and 'chatbot' components.\n",
        "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "    # Set the 'submit' event of the textbox to also call the 'respond' function.\n",
        "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
        "\n",
        "# Set the 'submit' event of the textbox to also call the 'respond' function.\n",
        "gr.close_all()\n",
        "# Launch the Gradio interface and make it shareable.\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "w1VFgdzQ7XEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding other advanced features"
      ],
      "metadata": {
        "id": "42RVzvPx_A91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Initialize the prompt with the system instruction.\n",
        "def format_chat_prompt(message, chat_history, instruction):\n",
        "    prompt = f\"System:{instruction}\"  # New: Initialize the prompt with the system instruction.\n",
        "    for turn in chat_history:\n",
        "        user_message, bot_message = turn\n",
        "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
        "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
        "    return prompt\n",
        "\n",
        "# Define a function to generate a response.\n",
        "def respond(message, chat_history, instruction, temperature=0.7):\n",
        "    prompt = format_chat_prompt(message, chat_history, instruction) # Format the prompt using the current message, chat history, and instruction.\n",
        "    chat_history = chat_history + [[message, \"\"]] # Append the current message to the chat history.\n",
        "    # Generate a response using the formatted prompt.\n",
        "    # The 'generate_stream' method is used for token streaming.\n",
        "    stream = client.generate_stream(prompt,\n",
        "                                      max_new_tokens=1024,\n",
        "                                      stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"],\n",
        "                                      temperature=temperature)\n",
        "                                      #stop_sequences to not generate the user answer\n",
        "    acc_text = \"\"\n",
        "    #Streaming the tokens\n",
        "    # This loop iterates through the stream of responses, where each response represents a token generated by the model.\n",
        "    # idx keeps track of the index of the response in the stream.\n",
        "    for idx, response in enumerate(stream):\n",
        "            text_token = response.token.text #Extracts the text content of the current response token\n",
        "\n",
        "            if response.details: #Checks if the response has any details. If it does, it returns from the respond function\n",
        "                return\n",
        "            #This condition checks if it's the first token in the response (idx == 0) and if it starts with a space.\n",
        "            #If so, it removes the leading space from the text_token.\n",
        "            if idx == 0 and text_token.startswith(\" \"):\n",
        "                text_token = text_token[1:]\n",
        "\n",
        "            #Accumulates the current text_token into the acc_text variable. This is used to build up the full response text.\n",
        "            acc_text += text_token\n",
        "            #Retrieves the last turn from the chat_history list and converts it into a list. This list contains the user and assistant messages from the last turn.\n",
        "            last_turn = list(chat_history.pop(-1))\n",
        "            last_turn[-1] += acc_text #Appends the accumulated acc_text to the last turn's assistant message. This is done to construct a complete response.\n",
        "            chat_history = chat_history + [last_turn]# Adds the modified last turn back to the chat_history list, effectively updating the conversation history.\n",
        "            yield \"\", chat_history #Yields an empty string and the updated chat_history.\n",
        "            acc_text = \"\" # Resets the acc_text variable to an empty string for the next token. This ensures that each token is accumulated separately.\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
        "    msg = gr.Textbox(label=\"Prompt\")\n",
        "    with gr.Accordion(label=\"Advanced options\",open=False):# Create an accordion component with advanced options (initially closed).\n",
        "\n",
        "        # Create a textbox component for the system message with 2 lines of text.\n",
        "        #This is the default or initial value of the text field. When the system component is created, the text field contains the text specified as the default value. Users can later modify it.\n",
        "        #In this case, the default value is \"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers,\" which represents an example system message briefly describing the conversation and the behavior of the AI assistant.\n",
        "        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n",
        "         # Create a slider component labeled \"temperature\" for adjusting a parameter.\n",
        "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")# Create a clear button that clears specified components (msg and chatbot).\n",
        "\n",
        "    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n",
        "    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\n",
        "gr.close_all()\n",
        "demo.queue().launch(share=True)"
      ],
      "metadata": {
        "id": "wnbZqmzQ7XHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.close_all()"
      ],
      "metadata": {
        "id": "R9tAGuyL156g",
        "outputId": "589dd99c-8e78-4dc0-b6b1-b7e0e445a980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    }
  ]
}